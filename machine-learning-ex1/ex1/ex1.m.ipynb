{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%% Machine Learning Online Class - Exercise 1: Linear Regression\n",
    "\n",
    "%  Instructions\n",
    "%  ------------\n",
    "%\n",
    "%  This file contains code that helps you get started on the\n",
    "%  linear exercise. You will need to complete the following functions\n",
    "%  in this exericse:\n",
    "%\n",
    "%     warmUpExercise.m\n",
    "%     plotData.m\n",
    "%     gradientDescent.m\n",
    "%     computeCost.m\n",
    "%     gradientDescentMulti.m\n",
    "%     computeCostMulti.m\n",
    "%     featureNormalize.m\n",
    "%     normalEqn.m\n",
    "%\n",
    "%  For this exercise, you will not need to change any code in this file,\n",
    "%  or any other files other than those mentioned above.\n",
    "%\n",
    "% x refers to the population size in 10,000s\n",
    "% y refers to the profit in $10,000s\n",
    "%\n",
    "\n",
    "%% Initialization\n",
    "clear ; close all; clc\n",
    "\n",
    "%% ==================== Part 1: Basic Function ====================\n",
    "% Complete warmUpExercise.m\n",
    "fprintf('Running warmUpExercise ... \\n');\n",
    "fprintf('5x5 Identity Matrix: \\n');\n",
    "warmUpExercise()\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "\n",
    "%% ======================= Part 2: Plotting =======================\n",
    "fprintf('Plotting Data ...\\n')\n",
    "data = load('ex1data1.txt');\n",
    "X = data(:, 1); y = data(:, 2);\n",
    "m = length(y); % number of training examples\n",
    "\n",
    "% Plot Data\n",
    "% Note: You have to complete the code in plotData.m\n",
    "plotData(X, y);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% =================== Part 3: Cost and Gradient descent ===================\n",
    "\n",
    "X = [ones(m, 1), data(:,1)]; % Add a column of ones to x\n",
    "theta = zeros(2, 1); % initialize fitting parameters\n",
    "\n",
    "% Some gradient descent settings\n",
    "iterations = 1500;\n",
    "alpha = 0.01;\n",
    "\n",
    "fprintf('\\nTesting the cost function ...\\n')\n",
    "% compute and display initial cost\n",
    "J = computeCost(X, y, theta);\n",
    "fprintf('With theta = [0 ; 0]\\nCost computed = %f\\n', J);\n",
    "fprintf('Expected cost value (approx) 32.07\\n');\n",
    "\n",
    "% further testing of the cost function\n",
    "J = computeCost(X, y, [-1 ; 2]);\n",
    "fprintf('\\nWith theta = [-1 ; 2]\\nCost computed = %f\\n', J);\n",
    "fprintf('Expected cost value (approx) 54.24\\n');\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "fprintf('\\nRunning Gradient Descent ...\\n')\n",
    "% run gradient descent\n",
    "theta = gradientDescent(X, y, theta, alpha, iterations);\n",
    "\n",
    "% print theta to screen\n",
    "fprintf('Theta found by gradient descent:\\n');\n",
    "fprintf('%f\\n', theta);\n",
    "fprintf('Expected theta values (approx)\\n');\n",
    "fprintf(' -3.6303\\n  1.1664\\n\\n');\n",
    "\n",
    "% Plot the linear fit\n",
    "hold on; % keep previous plot visible\n",
    "plot(X(:,2), X*theta, '-')\n",
    "legend('Training data', 'Linear regression')\n",
    "hold off % don't overlay any more plots on this figure\n",
    "\n",
    "% Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = [1, 3.5] *theta;\n",
    "fprintf('For population = 35,000, we predict a profit of %f\\n',...\n",
    "    predict1*10000);\n",
    "predict2 = [1, 7] * theta;\n",
    "fprintf('For population = 70,000, we predict a profit of %f\\n',...\n",
    "    predict2*10000);\n",
    "\n",
    "fprintf('Program paused. Press enter to continue.\\n');\n",
    "pause;\n",
    "\n",
    "%% ============= Part 4: Visualizing J(theta_0, theta_1) =============\n",
    "fprintf('Visualizing J(theta_0, theta_1) ...\\n')\n",
    "\n",
    "% Grid over which we will calculate J\n",
    "theta0_vals = linspace(-10, 10, 100);\n",
    "theta1_vals = linspace(-1, 4, 100);\n",
    "\n",
    "% initialize J_vals to a matrix of 0's\n",
    "J_vals = zeros(length(theta0_vals), length(theta1_vals));\n",
    "\n",
    "% Fill out J_vals\n",
    "for i = 1:length(theta0_vals)\n",
    "    for j = 1:length(theta1_vals)\n",
    "\t  t = [theta0_vals(i); theta1_vals(j)];\n",
    "\t  J_vals(i,j) = computeCost(X, y, t);\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "% Because of the way meshgrids work in the surf command, we need to\n",
    "% transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals';\n",
    "% Surface plot\n",
    "figure;\n",
    "surf(theta0_vals, theta1_vals, J_vals)\n",
    "xlabel('\\theta_0'); ylabel('\\theta_1');\n",
    "\n",
    "% Contour plot\n",
    "figure;\n",
    "% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))\n",
    "xlabel('\\theta_0'); ylabel('\\theta_1');\n",
    "hold on;\n",
    "plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
